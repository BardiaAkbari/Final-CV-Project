{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Point Transformer - ModelNet10 Classification\n",
    "\n",
    "This notebook implements the Point Transformer architecture for 3D point cloud classification using the ModelNet10 dataset.\n",
    "\n",
    "**Workflow:**\n",
    "1. Download Dataset via `kagglehub`\n",
    "2. Data Preprocessing (Parse `.off` mesh files to Point Clouds)\n",
    "3. Model Architecture (Point Transformer)\n",
    "4. Training & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "# 1. Download Dataset\n",
    "print(\"Downloading ModelNet10 dataset...\")\n",
    "path = kagglehub.dataset_download(\"balraj98/modelnet10-princeton-3d-object-dataset\")\n",
    "print(\"Path to dataset files:\", path)\n",
    "\n",
    "# Fix path if it lands in a subfolder\n",
    "if os.path.exists(os.path.join(path, 'ModelNet10')):\n",
    "    DATA_PATH = os.path.join(path, 'ModelNet10')\n",
    "else:\n",
    "    DATA_PATH = path\n",
    "    \n",
    "print(f\"Target Data Path: {DATA_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Processing Utils\n",
    "Since the Kaggle dataset provides `.off` files (meshes), we need a helper to read them and sample points from the vertices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_off(file):\n",
    "    \"\"\"\n",
    "    Reads a .off file and returns vertices.\n",
    "    \"\"\"\n",
    "    if 'OFF' != file.readline().strip():\n",
    "        raise ValueError('Not a valid OFF header')\n",
    "    \n",
    "    n_verts, n_faces, n_dontknow = tuple([int(s) for s in file.readline().strip().split(' ')])\n",
    "    \n",
    "    verts = [[float(s) for s in file.readline().strip().split(' ')] for i_vert in range(n_verts)]\n",
    "    # We ignore faces for this simple point cloud implementation and just take vertices\n",
    "    # faces = [[int(s) for s in file.readline().strip().split(' ')][1:] for i_face in range(n_faces)]\n",
    "    \n",
    "    return np.array(verts)\n",
    "\n",
    "def pc_normalize(pc):\n",
    "    \"\"\"\n",
    "    Center and scale the point cloud to unit sphere.\n",
    "    \"\"\"\n",
    "    centroid = np.mean(pc, axis=0)\n",
    "    pc = pc - centroid\n",
    "    m = np.max(np.sqrt(np.sum(pc**2, axis=1)))\n",
    "    pc = pc / m\n",
    "    return pc\n",
    "\n",
    "def farthest_point_sample(point, npoint):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        point: pointcloud data, [N, D]\n",
    "        npoint: number of samples\n",
    "    Return:\n",
    "        centroids: sampled pointcloud data, [npoint, D]\n",
    "    \"\"\"\n",
    "    N, D = point.shape\n",
    "    xyz = point[:,:3]\n",
    "    centroids = np.zeros((npoint, D))\n",
    "    distance = np.ones((N,)) * 1e10\n",
    "    farthest = np.random.randint(0, N)\n",
    "    for i in range(npoint):\n",
    "        centroids[i] = point[farthest, :]\n",
    "        centroid = xyz[farthest, :]\n",
    "        dist = np.sum((xyz - centroid) ** 2, -1)\n",
    "        mask = dist < distance\n",
    "        distance[mask] = dist[mask]\n",
    "        farthest = np.argmax(distance, -1)\n",
    "    return centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelNet10Dataset(Dataset):\n",
    "    def __init__(self, root, num_point=1024, split='train'):\n",
    "        self.root = root\n",
    "        self.num_point = num_point\n",
    "        self.split = split\n",
    "        \n",
    "        self.catfile = os.path.join(self.root, 'modelnet10_shape_names.txt')\n",
    "        # The kaggle dataset might not have the shape_names.txt, so we infer from folders if missing\n",
    "        if not os.path.exists(self.catfile):\n",
    "            self.cat = [d for d in os.listdir(self.root) if os.path.isdir(os.path.join(self.root, d))]\n",
    "            self.cat.sort()\n",
    "        else:\n",
    "            self.cat = [line.rstrip() for line in open(self.catfile)]\n",
    "            \n",
    "        self.classes = dict(zip(self.cat, range(len(self.cat))))\n",
    "        \n",
    "        self.datapath = []\n",
    "        for shape_name in self.cat:\n",
    "            # Folder structure: root/shape_name/train/shape_name_0001.off\n",
    "            shape_dir = os.path.join(self.root, shape_name, split)\n",
    "            files = glob.glob(os.path.join(shape_dir, '*.off'))\n",
    "            for f in files:\n",
    "                self.datapath.append((shape_name, f))\n",
    "                \n",
    "        print(f\"Loaded {len(self.datapath)} {split} samples.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.datapath)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        shape_name, file_path = self.datapath[index]\n",
    "        cls_idx = self.classes[shape_name]\n",
    "        cls_idx = np.array([cls_idx]).astype(np.int64)\n",
    "        \n",
    "        # Read OFF file\n",
    "        with open(file_path, 'r') as f:\n",
    "            point_set = read_off(f).astype(np.float32)\n",
    "\n",
    "        # Sampling\n",
    "        # If we have fewer points than required, we repeat indices\n",
    "        if len(point_set) < self.num_point:\n",
    "            choice = np.random.choice(len(point_set), self.num_point, replace=True)\n",
    "            point_set = point_set[choice, :]\n",
    "        else:\n",
    "            # Use FPS or Random Sampling. FPS is slow for on-the-fly training.\n",
    "            # We use random sampling for speed in this demo, FPS is better for fixed preprocessing.\n",
    "            # Switch to FPS if you want higher quality but slower training.\n",
    "            # point_set = farthest_point_sample(point_set, self.num_point)\n",
    "            np.random.shuffle(point_set)\n",
    "            point_set = point_set[:self.num_point, :]\n",
    "\n",
    "        # Normalize\n",
    "        point_set[:, 0:3] = pc_normalize(point_set[:, 0:3])\n",
    "\n",
    "        return point_set, cls_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Point Transformer Architecture\n",
    "This section contains the core layers: `TransformerBlock`, `TransitionDown`, and the main `PointTransformerCls` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def square_distance(src, dst):\n",
    "    B, N, _ = src.shape\n",
    "    _, M, _ = dst.shape\n",
    "    dist = -2 * torch.matmul(src, dst.permute(0, 2, 1))\n",
    "    dist += torch.sum(src ** 2, -1).view(B, N, 1)\n",
    "    dist += torch.sum(dst ** 2, -1).view(B, 1, M)\n",
    "    return dist\n",
    "\n",
    "def index_points(points, idx):\n",
    "    device = points.device\n",
    "    B = points.shape[0]\n",
    "    view_shape = list(idx.shape)\n",
    "    view_shape[1:] = [1] * (len(view_shape) - 1)\n",
    "    repeat_shape = list(idx.shape)\n",
    "    repeat_shape[0] = 1\n",
    "    batch_indices = torch.arange(B, dtype=torch.long).to(device).view(view_shape).repeat(repeat_shape)\n",
    "    new_points = points[batch_indices, idx, :]\n",
    "    return new_points\n",
    "\n",
    "def farthest_point_sample_tensor(xyz, npoint):\n",
    "    device = xyz.device\n",
    "    B, N, C = xyz.shape\n",
    "    centroids = torch.zeros(B, npoint, dtype=torch.long).to(device)\n",
    "    distance = torch.ones(B, N).to(device) * 1e10\n",
    "    farthest = torch.randint(0, N, (B,), dtype=torch.long).to(device)\n",
    "    batch_indices = torch.arange(B, dtype=torch.long).to(device)\n",
    "    for i in range(npoint):\n",
    "        centroids[:, i] = farthest\n",
    "        centroid = xyz[batch_indices, farthest, :].view(B, 1, 3)\n",
    "        dist = torch.sum((xyz - centroid) ** 2, -1)\n",
    "        mask = dist < distance\n",
    "        distance[mask] = dist[mask]\n",
    "        farthest = torch.max(distance, -1)[1]\n",
    "    return centroids\n",
    "\n",
    "def knn_point(nsample, xyz, new_xyz):\n",
    "    sqrdists = square_distance(new_xyz, xyz)\n",
    "    _, group_idx = torch.topk(sqrdists, nsample, dim=-1, largest=False, sorted=False)\n",
    "    return group_idx\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_points, d_model, k) -> None:\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(d_points, d_model)\n",
    "        self.fc2 = nn.Linear(d_model, d_points)\n",
    "        self.fc_delta = nn.Sequential(\n",
    "            nn.Linear(3, d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_model, d_model)\n",
    "        )\n",
    "        self.fc_gamma = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_model, d_model)\n",
    "        )\n",
    "        self.w_qs = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.w_ks = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.w_vs = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.k = k\n",
    "\n",
    "    def forward(self, xyz, features):\n",
    "        dists = square_distance(xyz, xyz)\n",
    "        knn_idx = dists.argsort()[:, :, :self.k]  # b x n x k\n",
    "        knn_xyz = index_points(xyz, knn_idx)\n",
    "        \n",
    "        pre = features\n",
    "        x = self.fc1(features)\n",
    "        q, k, v = self.w_qs(x), index_points(self.w_ks(x), knn_idx), index_points(self.w_vs(x), knn_idx)\n",
    "        pos_enc = self.fc_delta(xyz[:, :, None] - knn_xyz)  # b x n x k x f\n",
    "        \n",
    "        attn = self.fc_gamma(q[:, :, None] - k + pos_enc)\n",
    "        attn = F.softmax(attn / np.sqrt(k.size(-1)), dim=-2)  # b x n x k x f\n",
    "        \n",
    "        res = torch.einsum('bmnf,bmnf->bmf', attn, v + pos_enc)\n",
    "        res = self.fc2(res) + pre\n",
    "        return res, attn\n",
    "\n",
    "class PointTransformerCls(nn.Module):\n",
    "    def __init__(self, num_class, num_point=1024, input_dim=3, k=16):\n",
    "        super().__init__()\n",
    "        self.num_point = num_point\n",
    "        self.k = k\n",
    "        self.input_dim = input_dim\n",
    "        \n",
    "        # Initial embedding\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(input_dim, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 32)\n",
    "        )\n",
    "        \n",
    "        # Blocks and Transitions\n",
    "        self.transformer1 = TransformerBlock(32, 512, k)\n",
    "        self.trans1_stride = 4\n",
    "        self.trans1_fc = nn.Sequential(nn.Linear(32, 64), nn.ReLU(), nn.Linear(64, 64))\n",
    "        \n",
    "        self.transformer2 = TransformerBlock(64, 512, k)\n",
    "        self.trans2_stride = 4\n",
    "        self.trans2_fc = nn.Sequential(nn.Linear(64, 128), nn.ReLU(), nn.Linear(128, 128))\n",
    "        \n",
    "        self.transformer3 = TransformerBlock(128, 512, k)\n",
    "        self.trans3_stride = 4\n",
    "        self.trans3_fc = nn.Sequential(nn.Linear(128, 256), nn.ReLU(), nn.Linear(256, 256))\n",
    "        \n",
    "        self.transformer4 = TransformerBlock(256, 512, k)\n",
    "        self.trans4_stride = 4\n",
    "        self.trans4_fc = nn.Sequential(nn.Linear(256, 512), nn.ReLU(), nn.Linear(512, 512))\n",
    "        \n",
    "        self.transformer5 = TransformerBlock(512, 512, k)\n",
    "        \n",
    "        # Classification Head\n",
    "        self.fc_layer = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, num_class)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is (B, C, N). Transformer expects (B, N, C) for features and (B, N, 3) for xyz\n",
    "        xyz = x[:, :3, :].permute(0, 2, 1)\n",
    "        features = x.permute(0, 2, 1)\n",
    "        if self.input_dim == 3:\n",
    "            features = features[:, :, :3]\n",
    "        \n",
    "        features = self.fc1(features)\n",
    "        features, _ = self.transformer1(xyz, features)\n",
    "        \n",
    "        # Downsample 1\n",
    "        n_pt1 = xyz.shape[1] // self.trans1_stride\n",
    "        fps_idx1 = farthest_point_sample_tensor(xyz, n_pt1)\n",
    "        new_xyz1 = index_points(xyz, fps_idx1)\n",
    "        knn_idx1 = knn_point(self.k, xyz, new_xyz1)\n",
    "        grouped_features1 = index_points(features, knn_idx1)\n",
    "        grouped_features1 = self.trans1_fc(grouped_features1)\n",
    "        new_features1 = torch.max(grouped_features1, dim=2)[0]\n",
    "        xyz, features = new_xyz1, new_features1\n",
    "        \n",
    "        features, _ = self.transformer2(xyz, features)\n",
    "        \n",
    "        # Downsample 2\n",
    "        n_pt2 = xyz.shape[1] // self.trans2_stride\n",
    "        fps_idx2 = farthest_point_sample_tensor(xyz, n_pt2)\n",
    "        new_xyz2 = index_points(xyz, fps_idx2)\n",
    "        knn_idx2 = knn_point(self.k, xyz, new_xyz2)\n",
    "        grouped_features2 = index_points(features, knn_idx2)\n",
    "        grouped_features2 = self.trans2_fc(grouped_features2)\n",
    "        new_features2 = torch.max(grouped_features2, dim=2)[0]\n",
    "        xyz, features = new_xyz2, new_features2\n",
    "        \n",
    "        features, _ = self.transformer3(xyz, features)\n",
    "        \n",
    "        # Downsample 3\n",
    "        n_pt3 = xyz.shape[1] // self.trans3_stride\n",
    "        fps_idx3 = farthest_point_sample_tensor(xyz, n_pt3)\n",
    "        new_xyz3 = index_points(xyz, fps_idx3)\n",
    "        knn_idx3 = knn_point(self.k, xyz, new_xyz3)\n",
    "        grouped_features3 = index_points(features, knn_idx3)\n",
    "        grouped_features3 = self.trans3_fc(grouped_features3)\n",
    "        new_features3 = torch.max(grouped_features3, dim=2)[0]\n",
    "        xyz, features = new_xyz3, new_features3\n",
    "        \n",
    "        features, _ = self.transformer4(xyz, features)\n",
    "        \n",
    "        # Downsample 4\n",
    "        n_pt4 = xyz.shape[1] // self.trans4_stride\n",
    "        fps_idx4 = farthest_point_sample_tensor(xyz, n_pt4)\n",
    "        new_xyz4 = index_points(xyz, fps_idx4)\n",
    "        knn_idx4 = knn_point(self.k, xyz, new_xyz4)\n",
    "        grouped_features4 = index_points(features, knn_idx4)\n",
    "        grouped_features4 = self.trans4_fc(grouped_features4)\n",
    "        new_features4 = torch.max(grouped_features4, dim=2)[0]\n",
    "        xyz, features = new_xyz4, new_features4\n",
    "        \n",
    "        features, _ = self.transformer5(xyz, features)\n",
    "        \n",
    "        # Global Avg Pooling\n",
    "        features = torch.mean(features, dim=1)\n",
    "        x = self.fc_layer(features)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    batch_size = 16\n",
    "    epoch = 50   # Set to 200 for full convergence\n",
    "    learning_rate = 0.05\n",
    "    num_point = 1024\n",
    "    num_category = 10\n",
    "    optimizer = 'SGD'\n",
    "    decay_rate = 1e-4\n",
    "\n",
    "def train():\n",
    "    args = Args()\n",
    "    \n",
    "    # Check GPU\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Datasets\n",
    "    train_dataset = ModelNet10Dataset(root=DATA_PATH, num_point=args.num_point, split='train')\n",
    "    test_dataset = ModelNet10Dataset(root=DATA_PATH, num_point=args.num_point, split='test')\n",
    "    \n",
    "    trainDataLoader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=2, drop_last=True)\n",
    "    testDataLoader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False, num_workers=2)\n",
    "    \n",
    "    # Model\n",
    "    classifier = PointTransformerCls(num_class=args.num_category, num_point=args.num_point, input_dim=3).to(device)\n",
    "    criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = torch.optim.SGD(classifier.parameters(), lr=args.learning_rate, momentum=0.9, weight_decay=args.decay_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.7)\n",
    "    \n",
    "    best_instance_acc = 0.0\n",
    "    \n",
    "    print(\"Start Training...\")\n",
    "    for epoch in range(args.epoch):\n",
    "        classifier.train()\n",
    "        mean_correct = []\n",
    "        \n",
    "        # Training\n",
    "        for points, target in tqdm(trainDataLoader, desc=f'Epoch {epoch+1}/{args.epoch}'):\n",
    "            # Augmentation: Random Rotation\n",
    "            points = points.numpy()\n",
    "            points = np.transpose(points, (0, 2, 1)) # B, C, N\n",
    "            \n",
    "            theta = np.random.uniform(0, 2 * np.pi, size=points.shape[0])\n",
    "            rotation_matrix = np.zeros((points.shape[0], 3, 3))\n",
    "            rotation_matrix[:, 0, 0] = np.cos(theta)\n",
    "            rotation_matrix[:, 0, 2] = np.sin(theta)\n",
    "            rotation_matrix[:, 1, 1] = 1\n",
    "            rotation_matrix[:, 2, 0] = -np.sin(theta)\n",
    "            rotation_matrix[:, 2, 2] = np.cos(theta)\n",
    "            \n",
    "            points[:, :3, :] = np.matmul(rotation_matrix, points[:, :3, :])\n",
    "            \n",
    "            points = torch.Tensor(points).to(device)\n",
    "            target = target[:, 0].long().to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            pred = classifier(points)\n",
    "            loss = criterion(pred, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            pred_choice = pred.data.max(1)[1]\n",
    "            correct = pred_choice.eq(target.long().data).cpu().sum()\n",
    "            mean_correct.append(correct.item() / float(points.size()[0]))\n",
    "            \n",
    "        scheduler.step()\n",
    "        train_acc = np.mean(mean_correct)\n",
    "        \n",
    "        # Evaluation\n",
    "        with torch.no_grad():\n",
    "            classifier.eval()\n",
    "            total_correct = 0\n",
    "            total_seen = 0\n",
    "            for points, target in testDataLoader:\n",
    "                points = points.permute(0, 2, 1).float().to(device)\n",
    "                target = target[:, 0].long().to(device)\n",
    "                \n",
    "                pred = classifier(points)\n",
    "                pred_choice = pred.data.max(1)[1]\n",
    "                total_correct += pred_choice.eq(target.long().data).cpu().sum().item()\n",
    "                total_seen += points.size()[0]\n",
    "            \n",
    "            test_acc = total_correct / float(total_seen)\n",
    "            \n",
    "            if test_acc > best_instance_acc:\n",
    "                best_instance_acc = test_acc\n",
    "            \n",
    "            print(f'Epoch {epoch+1}: Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}, Best Test: {best_instance_acc:.4f}')\n",
    "\n",
    "train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
